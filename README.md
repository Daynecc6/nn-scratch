# Neural Network Derivation in Plain English

This explanation walks you through how a simple neural network works—from setting up its parameters to updating them using gradient descent.

---

## 1) Initialize Parameters

For each layer $$\ l \$$ in the network, we need to set up two things:

- **Weights $$\ W^l \$$:**  
  A matrix of size $$\ n_l \times n_{l-1} \$$, where $$\ n_l \$$ is the number of neurons in the current layer and $$\ n_{l-1} \$$ is the number in the previous layer. We initialize these weights to small random values (for example, drawn from a normal distribution with a small variance).

- **Biases $$\ b^l \$$:**  
  A vector of size $$\ n_l \times 1 \$$.

*Why?*  
This initialization gives the network a starting point. Random small weights help prevent any one neuron from dominating early on.

---

## 2) Forward Propagation

This is the process by which the network makes predictions.

For each layer $$\ l \$$:

1. **Compute the Linear Combination:**  
   Calculate a weighted sum of the inputs coming from the previous layer and add the bias:  
   $$\ Z^l = W^l A^{l-1} + b^l \$$  
   Here, $$\ A^{l-1} \$$ is the output from the previous layer with $$\ A^0 = X \$$ being the input data.

2. **Apply the Activation Function (Sigmoid):**  
   Transform the linear combination into a value between 0 and 1 using the sigmoid function:  
   $$\ A^l = \sigma(Z^l) = \frac{1}{1 + e^{-Z^l}} \$$

This single process (combining weights, bias, and activation) turns the input into a prediction for each layer:
$$\ A^l = \frac{1}{1 + e^{-(W^l A^{l-1} + b^l)}} \$$

*Why?*  
Forward propagation is how the network “guesses” the output. Each layer takes in its input, processes it, and passes it along until the final prediction is made.

---

## 3) Cost Function

For binary classification (i.e., when there are only two possible outcomes), we measure how well the network’s predictions match the true labels using the binary cross-entropy loss:

$$\ J = -\frac{1}{m} \sum_{i=1}^{m} \left[ Y_i \log(A^L_i) + \left(1 - Y_i\right)\log\left(1 - A^L_i\right) \right] \$$

- $$\ m \$$ is the number of training examples.
- $$\ A^L_i \$$ is the predicted probability for the $$\ i \$$ th example (from the output layer).
- $$\ Y_i \$$ is the true label (0 or 1).

*Why?*  
This cost function tells us how “wrong” the network’s predictions are. A lower cost means the network is doing a better job.

---

## 4) Backpropagation

Backpropagation is the process of figuring out how much each weight and bias contributed to the error (cost) so we know how to adjust them. We do this by calculating gradients (partial derivatives).

### 4.1 Output Layer Derivation

When working with the output layer, we need to determine how a change in its input (before the activation) affects the overall cost.

1. **Differentiate the Cost with Respect to the Output:**  
   First, we find the derivative of the cost function with respect to the network’s final output $$\ A^L \$$:
   $$\ \frac{\partial J}{\partial A^L} = -\frac{Y}{A^L} + \frac{1-Y}{1-A^L} \$$  
   This step shows how sensitive the cost is to the prediction.

2. **Include the Sigmoid Derivative:**  
   Because the output $$\ A^L \$$ is generated by a sigmoid function, we multiply by its derivative:
   $$\ \sigma'(Z^L) = A^L (1-A^L) \$$  
   This tells us how changes in the pre-activation value $$\ Z^L \$$ affect the output.

3. **Apply the Chain Rule:**  
   Combining these, we get:
   $$\ \frac{\partial J}{\partial Z^L} = \left(-\frac{Y}{A^L} + \frac{1-Y}{1-A^L}\right) \cdot A^L (1-A^L) \$$  
   This equation shows how the cost changes with a change in $$\ Z^L \$$.

4. **Simplify the Expression:**  
   We split the above expression into two parts to make it easier to understand:
   
   - **First term:**  
     $$\ -\frac{Y}{A^L} \cdot A^L (1-A^L) = -Y (1-A^L) \$$  
     *Why?* The $$\ A^L \$$ in the numerator cancels out, leaving a term that reflects the error when the true label is 1.
     
   - **Second term:**  
     $$\ \frac{1-Y}{1-A^L} \cdot A^L (1-A^L) = (1-Y) A^L \$$  
     *Why?* Here, the $$\ 1-A^L \$$ cancels out, showing the error when the true label is 0.

   Adding these together:
   $$\ -Y (1-A^L) + (1-Y) A^L = A^L - Y \$$

   **Result:**  
   $$\ dZ^L = A^L - Y \$$

   *Plain English:*  
   The term $$\ A^L - Y \$$ represents the difference between the network’s prediction and the actual label. If the prediction is too high, this value is positive; if it’s too low, it’s negative. This difference tells us how to adjust the weights and biases to reduce the error.

5. **Compute Gradients for the Output Layer Parameters:**  
   Now that we know how the cost changes with $$\ Z^L \$$, we determine the effect on the weights and biases:
   
   - **Weight Gradient:**  
     $$\ dW^L = \frac{1}{m}\ dZ^L \ (A^{L-1})^T \$$  
     *Why?* This tells us how much each weight contributed to the error by multiplying the error with the activations from the previous layer and then averaging over all examples.
     
   - **Bias Gradient:**  
     $$\ db^L = \frac{1}{m} \sum dZ^L \$$  
     *Why?* This is the average error for the bias, indicating how much to change it.

### 4.2 Hidden Layers Derivation

For each hidden layer $$\ l \$$ (where $$\ 1 \leq l < L \$$), we follow a similar process, but we must “pass” the error backward from the output layer.

1. **Backpropagate the Error:**  
   Compute how much the error from the next layer affects the current layer:
   $$\ dA^l = (W^{l+1})^T dZ^{l+1} \$$  
   *Why?* This step transfers the error backward through the network, showing the contribution of each neuron in the current layer.

2. **Compute the Local Gradient Using the Sigmoid Derivative:**  
   Adjust the backpropagated error by how sensitive the activation is:
   $$\ dZ^l = dA^l \cdot \sigma'(Z^l) = dA^l \cdot A^l (1-A^l) \$$  
   *Why?* This tells us how a small change in the input to the activation function affects the output of the current layer.

3. **Compute Gradients for the Layer’s Parameters:**  
   Determine how to change the weights and biases for this layer:
   
   - **Weight Gradient:**  
     $$\ dW^l = \frac{1}{m}\ dZ^l \ (A^{l-1})^T \$$  
     *Why?* It shows the impact of each weight on the error, averaged over all examples.
     
   - **Bias Gradient:**  
     $$\ db^l = \frac{1}{m} \sum dZ^l \$$  
     *Why?* It provides the average adjustment needed for the bias.

---

## 5) Parameter Update (Gradient Descent)

Once we have all the gradients, we update our weights and biases to reduce the cost. This is done using gradient descent:

$$\ W^l := W^l - \alpha\, dW^l \$$  
$$\ b^l := b^l - \alpha\, db^l \$$

*Why?*  
The parameter $$\ \alpha \$$ is called the learning rate, and it controls how big a step we take during each update. By subtracting a small portion of the gradient (which tells us the direction to reduce the error), we move the parameters toward values that should lower the overall cost.

---

## Summary

- **Initialization:**  
  We start by giving each layer small random weights and biases. This sets up our network with a basic starting point.

- **Forward Propagation:**  
  The network processes the input by multiplying it by weights, adding biases, and then applying a sigmoid function to get outputs. This is how it makes a prediction.

- **Cost Calculation:**  
  We use a measure called binary cross-entropy loss to determine how far the network’s predictions are from the true answers.

- **Backpropagation:**  
  We calculate how much each weight and bias contributed to the error by finding the derivative (or gradient) of the cost with respect to each parameter. For example, for the output layer, we end up with $$\ dZ^L = A^L - Y \$$—the difference between the prediction and the actual value.
  
- **Parameter Update:**  
  Finally, we adjust the weights and biases in the direction that reduces the error, using a method called gradient descent. This process repeats, gradually improving the network’s accuracy.

--- 

## 6) Implementation & Results

I implemented this neural network using Python and NumPy, training it on the **Breast Cancer dataset** from scikit-learn. This dataset contains features extracted from cell nuclei to classify tumors as **benign (0)** or **malignant (1)**.

The network consists of:
- **Input layer:** 30 features (corresponding to the dataset's input attributes)
- **Hidden layers:**  
  - 10 neurons in the first hidden layer
  - 5 neurons in the second hidden layer
- **Output layer:** 1 neuron with a sigmoid activation function (to output a probability)

### **Training Process**
1. **Data Preprocessing:**
   - The dataset is normalized so that all input features have a similar scale.
   - It is split into **80% training data** and **20% test data**.

2. **Network Training:**
   - The network is trained using **gradient descent** with a learning rate of 0.01.
   - The model is optimized over **100,000 iterations**.

3. **Predictions & Accuracy:**
   - After training, the network predicts whether tumors are benign or malignant.
   - The model achieves approximately **96% accuracy** on the test dataset.

This demonstrates that even a **basic neural network with two hidden layers** can effectively classify medical data when trained properly.



